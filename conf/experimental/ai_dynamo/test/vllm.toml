name = "vllm"
description = "vllm"
test_template_name = "AIDynamo"

[cmd_args]
docker_image_url = "nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.6.1.post1"

  [cmd_args.dynamo]
  backend = "vllm"

  [cmd_args.dynamo.prefill_worker]
  num-nodes = 1
  [cmd_args.dynamo.decode_worker]
  num-nodes = 1

  [cmd_args.genai_perf]
  endpoint = "v1/chat/completions"
  endpoint-type = "chat"
  extra-inputs = 'min_tokens:10'
  output-tokens-mean = 500
  output-tokens-stddev = 0
  random-seed = 123
  request-count = 20
  synthetic-input-tokens-mean = 3000
  synthetic-input-tokens-stddev = 0
  warmup-request-count = 10
  concurrency = 1
  extra-args = "--streaming -- -v --async"

[extra_env_vars]
UCX_LOG_LEVEL = "warn"
UCX_TLS = "cuda_copy,rc_x"
DYNAMO_NODELIST = "$(scontrol show hostname $SLURM_JOB_NODELIST | tr -s '\\n' ',')"
