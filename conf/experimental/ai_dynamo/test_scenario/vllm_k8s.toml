name = "vllm_k8s"

[[Tests]]
id = "dynamo.vllm"
test_name = "vllm"

  [Tests.cmd_args]
  docker_image_url = "nvcr.io/nvidia/ai-dynamo/vllm-runtime:0.6.1.post1"
  dynamo_graph_path = "conf/experimental/ai_dynamo/test/agg.yaml"
    [Tests.cmd_args.dynamo]
      [Tests.cmd_args.dynamo.prefill_worker]
      num-nodes = 1
      [Tests.cmd_args.dynamo.decode_worker]
      num-nodes = 1
    [Tests.cmd_args.genai_perf]
    model = "Qwen/Qwen3-0.6B"
    endpoint = "v1/chat/completions"
    endpoint-type = "chat"
    extra-inputs = 'min_tokens:10'
    output-tokens-mean = 500
    output-tokens-stddev = 0
    random-seed = 123
    request-count = 2
    synthetic-input-tokens-mean = 300
    synthetic-input-tokens-stddev = 0
    warmup-request-count = 1
    concurrency = 1
    extra-args = "--streaming -- -v --async"
