name = "llama"
description = "Llama2 70b"
test_template_name = "NeMoLauncher"
# FIX ME : ~training.model.position_embedding_type is a quick fix that should be changed with newer version of container
# the commit that should fix this issue is : 5b296e8af832c67d361fdfb80a165db3affaf76a
extra_cmd_args = "~training.model.position_embedding_type +training.model.fsdp=True ~training.model.optim.bucket_cap_mb ~training.model.optim.overlap_grad_sync ~training.model.optim.overlap_param_sync ~training.model.optim.contiguous_grad_buffer training.model.virtual_pipeline_model_parallel_size=null training.model.megatron_amp_O2=False training.model.activations_checkpoint_num_layers=null training.model.gradient_accumulation_fusion=False training.model.use_cpu_initialization=True training.model.optim.name=fused_adam training.model.tokenizer.model=TOKENIZER_MODEL training.exp_manager.create_wandb_logger=False"

[cmd_args]
"training" = "llama/llama2_70b"
"training.trainer.max_steps" = "120"
"training.model.global_batch_size" = "256"
"training.model.pipeline_model_parallel_size" = "1"