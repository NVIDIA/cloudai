# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES
# Copyright (c) 2024-2026 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

name = "dse_nemo_run_llama3_8b"

[[Tests]]
id = "dse_nemo_run_llama3_8b"
num_nodes = 1
time_limit = "00:60:00"

name = "dse_nemo_run_llama3_8b_fp8"
description = "dse_nemo_run_llama3_8b"
test_template_name = "NeMoRun"

  [Tests.cmd_args]
  docker_image_url = "nvcr.io#nvidia/nemo:25.11.01"
  task = "pretrain"
  recipe_name = "cloudai_llama3_8b_recipe"
  num_layers = 32

    [Tests.cmd_args.data]
    micro_batch_size = [1, 2]
    global_batch_size = 128

    [Tests.cmd_args.trainer]
    max_steps = 50
    val_check_interval = 50

      [Tests.cmd_args.trainer.strategy]
      tensor_model_parallel_size = 4
      pipeline_model_parallel_size = 1
      context_parallel_size = 1

      [Tests.cmd_args.trainer.plugins]
      fp8 = "hybrid"
      fp8_margin = 0
      fp8_amax_history_len = 1024
      fp8_amax_compute_algo = "max"
      fp8_params = true
      grad_reduce_in_fp32 = true

  [Tests.extra_env_vars]
  ENABLE_NUMA_CONTROL = "1"
  NCCL_P2P_NET_CHUNKSIZE = "2097152"
  TORCHX_MAX_RETRIES = "0"
  TRANSFORMERS_OFFLINE = "0"
  NCCL_NVLS_ENABLE = "0"
  NVTE_DP_AMAX_REDUCE_INTERVAL = "0"
  NVTE_ASYNC_AMAX_REDUCTION = "1"
  NVTE_FUSED_ATTN = "1"
  NVTE_FLASH_ATTN = "1"
  NEMO_LOG_MEMORY_USAGE = "1"
  CUDA_DEVICE_MAX_CONNECTIONS = "1"
  NVTE_FWD_LAYERNORM_SM_MARGIN = "16"
  NVTE_BWD_LAYERNORM_SM_MARGIN = "16"
