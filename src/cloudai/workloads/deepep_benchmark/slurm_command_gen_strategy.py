# SPDX-FileCopyrightText: NVIDIA CORPORATION & AFFILIATES
# Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from pathlib import Path
from typing import List, cast

from cloudai.systems.slurm import SlurmCommandGenStrategy

from .deepep_benchmark import DeepEPBenchmarkCmdArgs, DeepEPBenchmarkTestDefinition


class DeepEPBenchmarkSlurmCommandGenStrategy(SlurmCommandGenStrategy):
    """Command generation strategy for DeepEP benchmark on Slurm systems."""

    def _append_sbatch_directives(self, batch_script_content: List[str]) -> None:
        """Append SBATCH directives and head node IP detection for DeepEP."""
        super()._append_sbatch_directives(batch_script_content)
        batch_script_content.extend([
            "",
            "# Get head node information for torchrun",
            "nodes=( $( scontrol show hostnames $SLURM_JOB_NODELIST ) )",
            "nodes_array=($nodes)",
            "head_node=${nodes_array[0]}",
            'head_node_ip=$(srun --nodes=1 --ntasks=1 -w "$head_node" hostname --ip-address)',
            "",
            "echo Nodes: $SLURM_JOB_NODELIST",
            "echo Num Nodes: ${#nodes[@]}",
            "echo Head Node IP: $head_node_ip",
            ""
        ])

    def _container_mounts(self) -> List[str]:
        """Return container mounts specific to DeepEP benchmark."""
        tdef: DeepEPBenchmarkTestDefinition = cast(DeepEPBenchmarkTestDefinition, self.test_run.test)
        cmd_args: DeepEPBenchmarkCmdArgs = tdef.cmd_args

        # Create config file
        config_file_path = self.test_run.output_path / "config.yaml"
        self._generate_config_yaml(config_file_path, cmd_args)
        
        # Mount config file and results directory
        mounts = [
            f"{config_file_path.absolute()}:{cmd_args.config_file_path}",
            f"{self.test_run.output_path.absolute()}:{cmd_args.results_dir}",
        ]

        return mounts

    def image_path(self) -> str | None:
        """Return the Docker image path for DeepEP benchmark."""
        tdef: DeepEPBenchmarkTestDefinition = cast(DeepEPBenchmarkTestDefinition, self.test_run.test)
        return str(tdef.docker_image.installed_path)

    def generate_test_command(self) -> List[str]:
        """Generate the test command for DeepEP benchmark."""
        tdef: DeepEPBenchmarkTestDefinition = cast(DeepEPBenchmarkTestDefinition, self.test_run.test)
        cmd_args: DeepEPBenchmarkCmdArgs = tdef.cmd_args

        # Determine which benchmark script to run based on mode
        if cmd_args.mode == 'standard':
            benchmark_script = '/workspace/dp-benchmark/benchmark/benchmark.py'
        else:  # low_latency
            benchmark_script = '/workspace/dp-benchmark/benchmark/benchmark_ll.py'

        # Get number of nodes
        _, nodes = self.system.get_nodes_by_spec(self.test_run.nnodes, self.test_run.nodes)
        num_nodes = len(nodes) if nodes else self.test_run.nnodes

        # Build torchrun command
        command_parts = [
            "torchrun",
            f"--nnodes={num_nodes}",
            "--nproc_per_node=1",
            "--rdzv_id=$RANDOM",
            "--rdzv_backend=c10d",
            "--rdzv_endpoint=$head_node_ip:29500",
            benchmark_script,
            cmd_args.config_file_path
        ]

        return command_parts

    def _generate_config_yaml(self, config_path: Path, cmd_args: DeepEPBenchmarkCmdArgs) -> None:
        """
        Generate YAML configuration file for DeepEP benchmark.

        Args:
            config_path: Path where to write the config file.
            cmd_args: Command arguments for the benchmark.
        """
        config_lines = [
            "# DeepEP Benchmark Configuration",
            "# Generated by CloudAI",
            "",
            f"tokens: {cmd_args.tokens}",
            f"num_experts: {cmd_args.num_experts}",
            f"num_topk: {cmd_args.num_topk}",
            f"hidden_size: {cmd_args.hidden_size}",
            f"data_type: \"{cmd_args.data_type}\"",
            f"allow_nvlink_for_low_latency: {str(cmd_args.allow_nvlink_for_low_latency).lower()}",
            f"allow_mnnvl: {str(cmd_args.allow_mnnvl).lower()}",
            f"round_scale: {str(cmd_args.round_scale).lower()}",
            f"use_ue8m0: {str(cmd_args.use_ue8m0).lower()}",
            f"shuffle_columns: {str(cmd_args.shuffle_columns).lower()}",
            f"use_kineto_profiler: {str(cmd_args.use_kineto_profiler).lower()}",
            f"num_warmups: {cmd_args.num_warmups}",
            f"num_iterations: {cmd_args.num_iterations}",
        ]

        # Ensure parent directory exists
        config_path.parent.mkdir(parents=True, exist_ok=True)

        # Write config file
        with open(config_path, 'w') as f:
            f.write("\n".join(config_lines))

    def gen_srun_success_check(self) -> str:
        """Check if DeepEP benchmark completed successfully."""
        output_file = self.test_run.output_path / "stdout.txt"
        return f'grep -q "global_bw\\|deepep_time" {output_file} && echo 1 || echo 0'
