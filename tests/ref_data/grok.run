#!/bin/bash

export COMBINE_THRESHOLD=1
export PER_GPU_COMBINE_THRESHOLD=0
export XLA_FLAGS="--xla_disable_hlo_passes=rematerialization --xla_dump_hlo_pass_re=.* --xla_gpu_all_gather_combine_threshold_bytes=$COMBINE_THRESHOLD --xla_gpu_all_reduce_combine_threshold_bytes=$COMBINE_THRESHOLD --xla_gpu_disable_async_collectives=ALLREDUCE,ALLGATHER,REDUCESCATTER,COLLECTIVEBROADCAST,ALLTOALL,COLLECTIVEPERMUTE --xla_gpu_enable_all_gather_combine_by_dim=false --xla_gpu_enable_highest_priority_async_stream=true --xla_gpu_enable_latency_hiding_scheduler=false --xla_gpu_enable_pipelined_all_gather=true --xla_gpu_enable_pipelined_all_reduce=true --xla_gpu_enable_pipelined_reduce_scatter=true --xla_gpu_enable_reduce_scatter_combine_by_dim=false --xla_gpu_enable_triton_gemm=false --xla_gpu_enable_triton_softmax_fusion=false --xla_gpu_enable_while_loop_double_buffering=true --xla_gpu_graph_level=0 --xla_gpu_reduce_scatter_combine_threshold_bytes=$PER_GPU_COMBINE_THRESHOLD --xla_gpu_run_post_layout_collective_pipeliner=false"

if [ "$SLURM_NODEID" -eq 0 ] && [ "$SLURM_PROCID" -eq 0 ]; then
    nsys profile \
    -s none \
    -o /opt/paxml/workspace/nsys_profile_profile \
    --force-overwrite true \
    --capture-range=cudaProfilerApi \
    --capture-range-end=stop \
    --cuda-graph-trace=node \
    python3 -u -m paxml.main \
    --num_hosts=$SLURM_NTASKS \
    --server_addr=$SLURM_JOB_MASTER_NODE:12345 \
    --host_idx=$SLURM_PROCID \
    --job_log_dir=/opt/paxml/workspace/ \
    --tfds_data_dir=/opt/dataset \
    --enable_checkpoint_saving=False \
    --multiprocess_gpu \
    --alsologtostderr \
    --fdl_config="fdl/config" \
    --fdl.CHECKPOINT_POLICY=\"save_iteration_input\" \
    --fdl.COMBINE_QKV=False \
    --fdl.DCN_MESH_SHAPE='[1, 8, 1, 1]' \
    --fdl.DIMS_PER_HEAD=128 \
    --fdl.FPROP_DTYPE=\"bfloat16\" \
    --fdl.HIDDEN_DIMS=32768 \
    --fdl.ICI_MESH_SHAPE='[1, 1, 8, 1]' \
    --fdl.MAX_SEQ_LEN=8192 \
    --fdl.MAX_STEPS=20 \
    --fdl.MODEL_DIMS=6144 \
    --fdl.NUM_EXPERTS=8 \
    --fdl.NUM_GPUS=64 \
    --fdl.NUM_GROUPS=64 \
    --fdl.NUM_HEADS=48 \
    --fdl.NUM_KV_HEADS=8 \
    --fdl.NUM_LAYERS=64 \
    --fdl.NUM_MICROBATCHES=1 \
    --fdl.NUM_STAGES=1 \
    --fdl.PERCORE_BATCH_SIZE=1.0 \
    --fdl.USE_EXPERT_PARALLEL=True \
    --fdl.USE_FP8=1 \
    --fdl.USE_REPEATED_LAYER=False \
    --fdl.USE_TE_DPA=True \
    --fdl.VOCAB_SIZE=131072 >> /opt/paxml/workspace/profile_stderr_${SLURM_PROCID}.txt 2>&1
else
    python3 -u -m paxml.main \
    --num_hosts=$SLURM_NTASKS \
    --server_addr=$SLURM_JOB_MASTER_NODE:12345 \
    --host_idx=$SLURM_PROCID \
    --job_log_dir=/opt/paxml/workspace/ \
    --tfds_data_dir=/opt/dataset \
    --enable_checkpoint_saving=False \
    --multiprocess_gpu \
    --alsologtostderr \
    --fdl_config="fdl/config" \
    --fdl.CHECKPOINT_POLICY=\"save_iteration_input\" \
    --fdl.COMBINE_QKV=False \
    --fdl.DCN_MESH_SHAPE='[1, 8, 1, 1]' \
    --fdl.DIMS_PER_HEAD=128 \
    --fdl.FPROP_DTYPE=\"bfloat16\" \
    --fdl.HIDDEN_DIMS=32768 \
    --fdl.ICI_MESH_SHAPE='[1, 1, 8, 1]' \
    --fdl.MAX_SEQ_LEN=8192 \
    --fdl.MAX_STEPS=20 \
    --fdl.MODEL_DIMS=6144 \
    --fdl.NUM_EXPERTS=8 \
    --fdl.NUM_GPUS=64 \
    --fdl.NUM_GROUPS=64 \
    --fdl.NUM_HEADS=48 \
    --fdl.NUM_KV_HEADS=8 \
    --fdl.NUM_LAYERS=64 \
    --fdl.NUM_MICROBATCHES=1 \
    --fdl.NUM_STAGES=1 \
    --fdl.PERCORE_BATCH_SIZE=1.0 \
    --fdl.USE_EXPERT_PARALLEL=True \
    --fdl.USE_FP8=1 \
    --fdl.USE_REPEATED_LAYER=False \
    --fdl.USE_TE_DPA=True \
    --fdl.VOCAB_SIZE=131072 >> /opt/paxml/workspace/profile_stderr_${SLURM_PROCID}.txt 2>&1
fi

if [ "$SLURM_NODEID" -eq 0 ] && [ "$SLURM_PROCID" -eq 0 ]; then
    python /opt/jax/jax/tools/pgo_nsys_converter.py \
    --profile_path /opt/paxml/workspace/nsys_profile_profile.nsys-rep \
    --post_process \
    --pgle_output_path /opt/paxml/workspace/pgle_output_profile.pbtxt > /dev/null 2>&1
fi


export COMBINE_THRESHOLD=1
export PER_GPU_COMBINE_THRESHOLD=0
export XLA_FLAGS="--xla_disable_hlo_passes=rematerialization --xla_dump_hlo_pass_re=.* --xla_gpu_all_gather_combine_threshold_bytes=$COMBINE_THRESHOLD --xla_gpu_all_reduce_combine_threshold_bytes=$COMBINE_THRESHOLD --xla_gpu_enable_all_gather_combine_by_dim=false --xla_gpu_enable_highest_priority_async_stream=true --xla_gpu_enable_latency_hiding_scheduler=true --xla_gpu_enable_pipelined_all_gather=true --xla_gpu_enable_pipelined_all_reduce=true --xla_gpu_enable_pipelined_reduce_scatter=true --xla_gpu_enable_reduce_scatter_combine_by_dim=false --xla_gpu_enable_triton_gemm=false --xla_gpu_enable_triton_softmax_fusion=false --xla_gpu_enable_while_loop_double_buffering=true --xla_gpu_graph_level=0 --xla_gpu_pgle_profile_file_or_directory_path=/opt/paxml/workspace/pgle_output_profile.pbtxt --xla_gpu_reduce_scatter_combine_threshold_bytes=$PER_GPU_COMBINE_THRESHOLD --xla_gpu_run_post_layout_collective_pipeliner=false --xla_gpu_use_memcpy_local_p2p=false"

if [ "$SLURM_NODEID" -eq 0 ] && [ "$SLURM_PROCID" -eq 0 ]; then
    nsys profile \
    -s none \
    -o /opt/paxml/workspace/nsys_profile_perf \
    --force-overwrite true \
    --capture-range=cudaProfilerApi \
    --capture-range-end=stop \
    --cuda-graph-trace=node \
    python3 -u -m paxml.main \
    --num_hosts=$SLURM_NTASKS \
    --server_addr=$SLURM_JOB_MASTER_NODE:12345 \
    --host_idx=$SLURM_PROCID \
    --job_log_dir=/opt/paxml/workspace/ \
    --tfds_data_dir=/opt/dataset \
    --enable_checkpoint_saving=False \
    --multiprocess_gpu \
    --alsologtostderr \
    --fdl_config="fdl/config" \
    --fdl.CHECKPOINT_POLICY=\"save_iteration_input\" \
    --fdl.COMBINE_QKV=False \
    --fdl.DCN_MESH_SHAPE='[1, 8, 1, 1]' \
    --fdl.DIMS_PER_HEAD=128 \
    --fdl.FPROP_DTYPE=\"bfloat16\" \
    --fdl.HIDDEN_DIMS=32768 \
    --fdl.ICI_MESH_SHAPE='[1, 1, 8, 1]' \
    --fdl.MAX_SEQ_LEN=8192 \
    --fdl.MAX_STEPS=20 \
    --fdl.MODEL_DIMS=6144 \
    --fdl.NUM_EXPERTS=8 \
    --fdl.NUM_GPUS=64 \
    --fdl.NUM_GROUPS=64 \
    --fdl.NUM_HEADS=48 \
    --fdl.NUM_KV_HEADS=8 \
    --fdl.NUM_LAYERS=64 \
    --fdl.NUM_MICROBATCHES=1 \
    --fdl.NUM_STAGES=1 \
    --fdl.PERCORE_BATCH_SIZE=1.0 \
    --fdl.USE_EXPERT_PARALLEL=True \
    --fdl.USE_FP8=1 \
    --fdl.USE_REPEATED_LAYER=False \
    --fdl.USE_TE_DPA=True \
    --fdl.VOCAB_SIZE=131072
else
    python3 -u -m paxml.main \
    --num_hosts=$SLURM_NTASKS \
    --server_addr=$SLURM_JOB_MASTER_NODE:12345 \
    --host_idx=$SLURM_PROCID \
    --job_log_dir=/opt/paxml/workspace/ \
    --tfds_data_dir=/opt/dataset \
    --enable_checkpoint_saving=False \
    --multiprocess_gpu \
    --alsologtostderr \
    --fdl_config="fdl/config" \
    --fdl.CHECKPOINT_POLICY=\"save_iteration_input\" \
    --fdl.COMBINE_QKV=False \
    --fdl.DCN_MESH_SHAPE='[1, 8, 1, 1]' \
    --fdl.DIMS_PER_HEAD=128 \
    --fdl.FPROP_DTYPE=\"bfloat16\" \
    --fdl.HIDDEN_DIMS=32768 \
    --fdl.ICI_MESH_SHAPE='[1, 1, 8, 1]' \
    --fdl.MAX_SEQ_LEN=8192 \
    --fdl.MAX_STEPS=20 \
    --fdl.MODEL_DIMS=6144 \
    --fdl.NUM_EXPERTS=8 \
    --fdl.NUM_GPUS=64 \
    --fdl.NUM_GROUPS=64 \
    --fdl.NUM_HEADS=48 \
    --fdl.NUM_KV_HEADS=8 \
    --fdl.NUM_LAYERS=64 \
    --fdl.NUM_MICROBATCHES=1 \
    --fdl.NUM_STAGES=1 \
    --fdl.PERCORE_BATCH_SIZE=1.0 \
    --fdl.USE_EXPERT_PARALLEL=True \
    --fdl.USE_FP8=1 \
    --fdl.USE_REPEATED_LAYER=False \
    --fdl.USE_TE_DPA=True \
    --fdl.VOCAB_SIZE=131072
fi

if [ "$SLURM_NODEID" -eq 0 ] && [ "$SLURM_PROCID" -eq 0 ]; then
    python /opt/jax/jax/tools/pgo_nsys_converter.py \
    --profile_path /opt/paxml/workspace/nsys_profile_perf.nsys-rep \
    --post_process \
    --pgle_output_path /opt/paxml/workspace/pgle_output_perf.pbtxt > /dev/null 2>&1
fi